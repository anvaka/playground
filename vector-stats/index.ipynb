{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib transformers numpy sentence_transformers annoy nltk spacy graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import OrderedDict\n",
    "from annoy import AnnoyIndex\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def pre_process(sentences):\n",
    "    processed_sentences = [re.sub('\\s+', ' ', sentence).strip() for sentence in sentences]\n",
    "    # Remove duplicates but preserve order\n",
    "    processed_sentences = list(OrderedDict.fromkeys(processed_sentences))\n",
    "    return processed_sentences\n",
    "\n",
    "def split_into_sentences_ntlk(text):\n",
    "    nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    # Pre-process the sentences to standardize whitespace\n",
    "    sentences = pre_process(sentences)\n",
    "    return sentences\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    sentences = pre_process(sentences)\n",
    "    return sentences\n",
    "\n",
    "def generate_embeddings(sentences, model):\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "def build_annoy_index(embeddings, file_name='annoy_index.ann'):\n",
    "    dimension = embeddings[0].shape[0]\n",
    "    annoy_index = AnnoyIndex(dimension, 'angular')\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        annoy_index.add_item(i, embedding)\n",
    "\n",
    "    # Build the index\n",
    "    annoy_index.build(10)  # 10 trees\n",
    "    annoy_index.save(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'tom_sawyer.txt'\n",
    "# Read the file\n",
    "text = read_file(file_path)\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = split_into_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "# Load the transformer model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "embeddings = generate_embeddings(sentences, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_annoy_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy_index = AnnoyIndex(embeddings[0].shape[0], 'angular')\n",
    "annoy_index.load('annoy_index.ann')  # load saved index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_sentences(annoy_index, embeddings, num_nearest):\n",
    "    all_nearest_sentences = []\n",
    "    all_distances = []\n",
    "\n",
    "    for i, _ in enumerate(embeddings):\n",
    "        nearest_sentences = annoy_index.get_nns_by_item(i, num_nearest + 1)  # +1 because the sentence is closest to itself\n",
    "        nearest_sentences.remove(i)  # remove the sentence itself\n",
    "\n",
    "        distances = [annoy_index.get_distance(i, j) for j in nearest_sentences]\n",
    "        all_distances.extend(distances)\n",
    "\n",
    "        all_nearest_sentences.append(nearest_sentences)\n",
    "\n",
    "    return all_nearest_sentences, all_distances\n",
    "\n",
    "def plot_distance_distribution(all_distances):\n",
    "    plt.hist(all_distances, bins=30)\n",
    "    plt.title('Distribution of distances to nearest sentences')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "num_nearest = 42\n",
    "all_nearest_sentences, all_distances = get_nearest_sentences(annoy_index, embeddings, num_nearest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_distance_distribution(all_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indexes = sorted(range(len(all_distances)), key=lambda k: all_distances[k])\n",
    "\n",
    "# Print few most similar sentences\n",
    "for i in sorted_indexes[:150]:  # 10 can be replaced with the number of examples you want to print\n",
    "    sentence_index = i // num_nearest\n",
    "    nearest_sentence_index = all_nearest_sentences[sentence_index][i % num_nearest]\n",
    "\n",
    "    print(f\"Sentence: {sentences[sentence_index]}\")\n",
    "    print(f\"Most similar sentence: {sentences[nearest_sentence_index]}\")\n",
    "    print(f\"Distance: {all_distances[i]}\")\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'• You comply with all other terms of this agreement for free\\n        distribution of Project Gutenberg™ works.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[all_nearest_sentences[1][2 % num_nearest]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def create_graph_file(sentences, all_nearest_sentences, annoy_index, threshold, filename='graph.dot'):\n",
    "    dot = Digraph()\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        dot.node(str(i), sentence)\n",
    "    \n",
    "    for i, nearest_sentences in enumerate(all_nearest_sentences):\n",
    "        for j in nearest_sentences:\n",
    "            distance = annoy_index.get_distance(i, j)\n",
    "            if distance < threshold:\n",
    "                dot.edge(str(i), str(j), label=str(distance))\n",
    "                \n",
    "    dot.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph file\n",
    "create_graph_file(sentences, all_nearest_sentences, annoy_index, threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for each sentence, let's plot a line chart that shows distance from the sentence\n",
    "# to \"Happy\" and \"Sad\" respectively.\n",
    "# We will use the same color scheme as above.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the embeddings as provided in your code snippet\n",
    "\n",
    "# Define a happy sentence to compare with\n",
    "happy_sentence = \"I am very happy!\"\n",
    "happy_embedding = model.encode([happy_sentence])\n",
    "\n",
    "# Calculate the cosine similarity between the happy embedding and each sentence embedding\n",
    "distances = [cosine_similarity([embedding], [happy_embedding[0]])[0][0] for embedding in embeddings]\n",
    "\n",
    "# Plot the distances\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Sentence Index')\n",
    "plt.ylabel('Cosine Similarity to Happy Sentiment')\n",
    "plt.title('Distance Between Sentences and Happy Sentiment')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Nearest Sentences:\n",
      "I’m satisfied with it.\n",
      "Sid seemed satisfied.\n",
      "The neighboring spectators shook with a gentle inward joy, several faces went behind fans and hand-kerchiefs, and Tom was entirely happy.\n",
      "\n",
      "Top 3 Farthest Sentences:\n",
      "She had sunk into a dreary apathy and would not be roused.\n",
      "The choir always tittered and whispered all through service.\n",
      "Their speed was slow, however, because pitfalls were somewhat common, and had to be guarded against.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert distances to a numpy array for easy manipulation\n",
    "distances_array = np.array(distances)\n",
    "\n",
    "# Get the indices of the top 3 nearest and farthest sentences\n",
    "nearest_indices = distances_array.argsort()[-3:][::-1]\n",
    "farthest_indices = distances_array.argsort()[:3]\n",
    "\n",
    "# Retrieve the corresponding sentences\n",
    "nearest_sentences = [sentences[i] for i in nearest_indices]\n",
    "farthest_sentences = [sentences[i] for i in farthest_indices]\n",
    "\n",
    "print(\"Top 3 Nearest Sentences:\")\n",
    "for sentence in nearest_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"\\nTop 3 Farthest Sentences:\")\n",
    "for sentence in farthest_sentences:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
