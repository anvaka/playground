{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib transformers numpy sentence_transformers annoy nltk spacy graphviz pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import OrderedDict\n",
    "from annoy import AnnoyIndex\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def pre_process(sentences):\n",
    "    processed_sentences = [re.sub('\\s+', ' ', sentence).strip() for sentence in sentences]\n",
    "    # Remove duplicates but preserve order\n",
    "    processed_sentences = list(OrderedDict.fromkeys(processed_sentences))\n",
    "    return processed_sentences\n",
    "\n",
    "def split_into_sentences_ntlk(text):\n",
    "    nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    # Pre-process the sentences to standardize whitespace\n",
    "    sentences = pre_process(sentences)\n",
    "    return sentences\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    sentences = pre_process(sentences)\n",
    "    return sentences\n",
    "\n",
    "def generate_embeddings(sentences, model):\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "def build_annoy_index(embeddings, file_name='pdf_annoy_index.ann'):\n",
    "    dimension = embeddings[0].shape[0]\n",
    "    annoy_index = AnnoyIndex(dimension, 'angular')\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        annoy_index.add_item(i, embedding)\n",
    "\n",
    "    # Build the index\n",
    "    annoy_index.build(10)  # 10 trees\n",
    "    annoy_index.save(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "text = extract_text(\"1706.03762.pdf\")\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = split_into_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "# Load the transformer model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "embeddings = generate_embeddings(sentences, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_annoy_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy_index = AnnoyIndex(embeddings[0].shape[0], 'angular')\n",
    "annoy_index.load('pdf_annoy_index.ann')  # load saved index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_sentences(annoy_index, embeddings, num_nearest):\n",
    "    all_nearest_sentences = []\n",
    "    all_distances = []\n",
    "\n",
    "    for i, _ in enumerate(embeddings):\n",
    "        nearest_sentences = annoy_index.get_nns_by_item(i, num_nearest + 1)  # +1 because the sentence is closest to itself\n",
    "        nearest_sentences.remove(i)  # remove the sentence itself\n",
    "\n",
    "        distances = [annoy_index.get_distance(i, j) for j in nearest_sentences]\n",
    "        all_distances.extend(distances)\n",
    "\n",
    "        all_nearest_sentences.append(nearest_sentences)\n",
    "\n",
    "    return all_nearest_sentences, all_distances\n",
    "\n",
    "def plot_distance_distribution(all_distances):\n",
    "    plt.hist(all_distances, bins=30)\n",
    "    plt.title('Distribution of distances to nearest sentences')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "num_nearest = 42\n",
    "all_nearest_sentences, all_distances = get_nearest_sentences(annoy_index, embeddings, num_nearest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_distance_distribution(all_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indexes = sorted(range(len(all_distances)), key=lambda k: all_distances[k])\n",
    "\n",
    "# Print few most similar sentences\n",
    "for i in sorted_indexes[:150]:  # 10 can be replaced with the number of examples you want to print\n",
    "    sentence_index = i // num_nearest\n",
    "    nearest_sentence_index = all_nearest_sentences[sentence_index][i % num_nearest]\n",
    "\n",
    "    print(f\"Sentence: {sentences[sentence_index]}\")\n",
    "    print(f\"Most similar sentence: {sentences[nearest_sentence_index]}\")\n",
    "    print(f\"Distance: {all_distances[i]}\")\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def create_graph_file(sentences, all_nearest_sentences, annoy_index, threshold, filename='graph.dot'):\n",
    "    dot = Digraph()\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        dot.node(str(i), sentence)\n",
    "    \n",
    "    for i, nearest_sentences in enumerate(all_nearest_sentences):\n",
    "        for j in nearest_sentences:\n",
    "            distance = annoy_index.get_distance(i, j)\n",
    "            if distance < threshold:\n",
    "                dot.edge(str(i), str(j), label=str(distance))\n",
    "                \n",
    "    dot.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph_file(sentences, all_nearest_sentences, annoy_index, threshold=0.9, filename='pdfgraph.dot')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
